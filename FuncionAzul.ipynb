{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8a875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/arturo/Downloads/')\n",
    "\n",
    "from Funciones import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffad5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from Funciones import *\n",
    "# from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "# from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "# from pyspark.sql.functions import col,array_contains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebed985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 23:24:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d318af",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e715f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi4 = spark.read.csv(\"DATA_SIC_ENTIDADES_PUBLICAS_AYUDAS_ESP.csv\", sep=\";\", header=True)\n",
    "a12 = spark.read.csv(\"DATA_RELACION_ENTIDADES_MAESTRO.csv\", sep=\";\", header=True)\n",
    "fi5 = spark.read.csv(\"DATA_SIC_AYUDA_ESP_UO_PB_J_MAESTRO.csv\", sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390fb48",
   "metadata": {},
   "source": [
    "# Cruce 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc76ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruce1 = fi5.join(a12,fi5.Id ==  a12.Id_NIVEL_0,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304cf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruce1 = cruce1.withColumn(\"Similitud\", \n",
    "                           udf_Distance_ratcliff_obershelp(\n",
    "                               UDF_normalizarTexto(cruce1.NOMBRE_ENTIDAD_NIVEL_1),\n",
    "                               cruce1.Centro_Norm\n",
    "                           )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525feb3",
   "metadata": {},
   "source": [
    "# DATA_SIC_ENTIDADES_CENTROS_PB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2214f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "maxId = fi4.withColumn(\"Id\", fi4.Id.cast('int')).select(\"Id\").rdd.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04cca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cruce1.filter(\"Similitud <0.875\")\\\n",
    "             .withColumn(\"CIF\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"Entidad_Norm\",col(\"Centro_Norm\"))\\\n",
    "             .withColumn(\"Provincia_Entidad\",col(\"Provincia_Centro\"))\\\n",
    "             .withColumn(\"ID_ENTIDAD\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"NIF_COD\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"ACRONIMO\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"NOMBRE_ENTIDAD\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"Nombre_Entidad_Mostrar\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"TIPO_ENTIDAD_N1_1\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"TIPO_ENTIDAD_N2_1\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"DIRECCION_POSTAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"COD_POSTAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"COD_PROVINCIA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"PROVINCIA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"COD_CCAA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"CCAA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"ENLACE_WEB\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"SOMMA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"TIPO_ENTIDAD_REGIONAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"ESTADO_x\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"CodigoInvente\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"DenominacionSocial\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"FormaJuridica_Codigo\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"FormaJuridica_Descripcion\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"NIF\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"codigoDir3\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"codigoOrigen\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"Provincia_Codigo\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_UD_ORGANICA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_DNM_UD_ORGANICA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_NIVEL_ADMON\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_TIPO_ENT_PUBLICA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"N_NIVEL_JERARQUICO\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_DEP_UD_SUPERIOR\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_DNM_UD_ORGANICA_SUPERIOR\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_DEP_UD_PRINCIPAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_DNM_UD_ORGANICA_PRINCIPAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"B_SW_DEP_EDP_PRINCIPAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_DEP_EDP_PRINCIPAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_DNM_UD_ORGANICA_EDP_PRINCIPAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_ESTADO\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"D_VIG_ALTA_OFICIAL\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"NIF_CIF\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_ID_AMB_PROVINCIA\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"C_DESC_PROV\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"CONTACTOS\",lit(None).cast(StringType()))\\\n",
    "             .withColumn(\"List_Entidad_Norm\",col(\"Centro_Norm\"))\\\n",
    "             .withColumn(\"List_Provincia_Entidad\",col(\"Provincia_Centro\"))\\\n",
    "             .withColumn(\"List_CIF\",lit(None).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b811ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropDuplicates([\"Entidad_Norm\", \"Provincia_Entidad\"])\n",
    "data = data.drop('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4086cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Index = list(range(maxId+ 1,maxId + data.count() + 1))\n",
    "Index = spark.createDataFrame(Index, IntegerType())\n",
    "Index = Index.withColumnRenamed(\"value\", \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f27586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"Dummy\", monotonically_increasing_id())\n",
    "Index = Index.withColumn(\"Dummy\", monotonically_increasing_id())\n",
    "data = data.join(Index, \"Dummy\", \"outer\").drop(\"Dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a83d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data.select(\"Id\",\n",
    "                    \"CIF\",\n",
    "                    \"Entidad_Norm\",\n",
    "                    \"Provincia_Entidad\",\n",
    "                    \"ID_ENTIDAD\",\n",
    "                    \"NIF_COD\",\n",
    "                    \"ACRONIMO\",\n",
    "                    \"NOMBRE_ENTIDAD\",\n",
    "                    \"Nombre_Entidad_Mostrar\",\n",
    "                    \"TIPO_ENTIDAD_N1_1\",\n",
    "                    \"TIPO_ENTIDAD_N2_1\",\n",
    "                    \"DIRECCION_POSTAL\",\n",
    "                    \"COD_POSTAL\",\n",
    "                    \"COD_PROVINCIA\",\n",
    "                    \"PROVINCIA\",\n",
    "                    \"COD_CCAA\",\n",
    "                    \"CCAA\",\n",
    "                    \"ENLACE_WEB\",\n",
    "                    \"SOMMA\",\n",
    "                    \"TIPO_ENTIDAD_REGIONAL\",\n",
    "                    \"ESTADO_x\",\n",
    "                    \"CodigoInvente\",\n",
    "                    \"DenominacionSocial\",\n",
    "                    \"FormaJuridica_Codigo\",\n",
    "                    \"FormaJuridica_Descripcion\",\n",
    "                    \"NIF\",\n",
    "                    \"codigoDir3\",\n",
    "                    \"codigoOrigen\",\n",
    "                    \"Provincia_Codigo\",\n",
    "                    \"C_ID_UD_ORGANICA\",\n",
    "                    \"C_DNM_UD_ORGANICA\",\n",
    "                    \"C_ID_NIVEL_ADMON\",\n",
    "                    \"C_ID_TIPO_ENT_PUBLICA\",\n",
    "                    \"N_NIVEL_JERARQUICO\",\n",
    "                    \"C_ID_DEP_UD_SUPERIOR\",\n",
    "                    \"C_DNM_UD_ORGANICA_SUPERIOR\",\n",
    "                    \"C_ID_DEP_UD_PRINCIPAL\",\n",
    "                    \"C_DNM_UD_ORGANICA_PRINCIPAL\",\n",
    "                    \"B_SW_DEP_EDP_PRINCIPAL\",\n",
    "                    \"C_ID_DEP_EDP_PRINCIPAL\",\n",
    "                    \"C_DNM_UD_ORGANICA_EDP_PRINCIPAL\",\n",
    "                    \"C_ID_ESTADO\",\n",
    "                    \"D_VIG_ALTA_OFICIAL\",\n",
    "                    \"NIF_CIF\",\n",
    "                    \"C_ID_AMB_PROVINCIA\",\n",
    "                    \"C_DESC_PROV\",\n",
    "                    \"CONTACTOS\",\n",
    "                    \"List_Entidad_Norm\",\n",
    "                    \"List_Provincia_Entidad\",\n",
    "                    \"List_CIF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22dd956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_1.write.option(\"header\",True).format(\"csv\").save(\"/Nombre_Intermedio_cruce1_entidades/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd53bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=====>                                                   (1 + 9) / 10]\r"
     ]
    }
   ],
   "source": [
    "data_1.write.format(\"parquet\").save(\"/Nombre_Intermedio_cruce1_entidades_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8708fbc",
   "metadata": {},
   "source": [
    "# Nombre_Output_investigadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdbf3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = cruce1.filter(\"Id_NIVEL_0 is not NULL and Id is not NULL and Similitud >=0.875\")\\\n",
    "        .withColumn(\"Final_Score\", col(\"Similitud\"))\\\n",
    "        .withColumn(\"Id_Centro\", col(\"Id_NIVEL_1\"))\\\n",
    "        .withColumn(\"Centro_Nombre_match\", col(\"NOMBRE_ENTIDAD_NIVEL_1\"))\\\n",
    "        .withColumn(\"Centro_Provincia_match\", col(\"COD_PROVINCIA_NIVEL_1\"))\\\n",
    "        .withColumn(\"Similitud_Centro\", col(\"Similitud\"))\\\n",
    "        .select(fi5.columns + [\"Final_Score\",\"Id_Centro\", \"Centro_Nombre_match\", \"Centro_Provincia_match\", \"Similitud_Centro\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196406af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = data.withColumn(\"Final_Score\", col(\"Similitud\"))\\\n",
    "        .withColumn(\"Id_Centro\", col(\"Id\"))\\\n",
    "        .withColumn(\"Centro_Nombre_match\", col(\"NOMBRE_ENTIDAD_NIVEL_1\"))\\\n",
    "        .withColumn(\"Centro_Provincia_match\", col(\"COD_PROVINCIA_NIVEL_1\"))\\\n",
    "        .withColumn(\"Similitud_Centro\", col(\"Similitud\"))\\\n",
    "        .select(fi5.columns + [\"Final_Score\",\"Id_Centro\", \"Centro_Nombre_match\", \"Centro_Provincia_match\", \"Similitud_Centro\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec4df652",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_2.union(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0b40c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 23:34:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/07/20 23:34:35 ERROR FileOutputCommitter: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 23:40:30 ERROR Executor: Exception in task 10.0 in stage 20.0 (TID 46)\n",
      "java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_202207202340292600958027802038332_0020_m_000010_46 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolE"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                       (0 + 10) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/07/20 23:40:30 ERROR Executor: Exception in task 11.0 in stage 20.0 (TID 47)\n",
      "java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/07/20 23:40:30 WARN TaskSetManager: Lost task 11.0 in stage 20.0 (TID 47) (10.0.0.103 executor driver): java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/07/20 23:40:30 ERROR TaskSetManager: Task 11 in stage 20.0 failed 1 times; aborting job\n",
      "22/07/20 23:40:30 WARN TaskSetManager: Lost task 10.0 in stage 20.0 (TID 46) (10.0.0.103 executor driver): java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_202207202340292600958027802038332_0020_m_000010_46 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/07/20 23:40:30 ERROR FileFormatWriter: Aborting job 8816f83a-7986-4e50-9796-bb0326c8f908.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 20.0 failed 1 times, most recent failure: Lost task 11.0 in stage 20.0 (TID 47) (10.0.0.103 executor driver): java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o674.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 20.0 failed 1 times, most recent failure: Lost task 11.0 in stage 20.0 (TID 47) (10.0.0.103 executor driver): java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 41 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fy/cjds5kvd3692g8__1nlvytj00000gn/T/ipykernel_52640/2583121657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Nombre_Output_investigadores/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o674.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 20.0 failed 1 times, most recent failure: Lost task 11.0 in stage 20.0 (TID 47) (10.0.0.103 executor driver): java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 41 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/Nombre_Output_investigadores/_temporary/0/_temporary/attempt_20220720234029384137653949799326_0020_m_000011_47 (exists=false, cwd=file:/Users/arturo/Documents/AVALON)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "result.write.option(\"header\",True).format(\"csv\").save(\"/Users/arturo/repos/repos/test1/data-eng-scripts/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140b2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 23:40:32 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 48) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                        (0 + 9) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 23:40:32 WARN TaskSetManager: Lost task 7.0 in stage 20.0 (TID 55) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 1.0 in stage 20.0 (TID 49) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                        (0 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 9.0 in stage 20.0 (TID 57) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 8.0 in stage 20.0 (TID 56) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 2.0 in stage 20.0 (TID 50) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 3.0 in stage 20.0 (TID 51) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 5.0 in stage 20.0 (TID 53) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 6.0 in stage 20.0 (TID 54) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n",
      "22/07/20 23:40:33 WARN TaskSetManager: Lost task 4.0 in stage 20.0 (TID 52) (10.0.0.103 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                        (0 + 2) / 12]\r"
     ]
    }
   ],
   "source": [
    "result.write.format(\"parquet\").save(\"/Nombre_Output_investigadores/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa018a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c13542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd52b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded24963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdb06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07953b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a53a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44068e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb953922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df74b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1e2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b959a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf418ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec785fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5271c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06031f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babdddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa11667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585e663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa6f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158b49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7854ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ec90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"Id\", sequence(lit(maxId+1), lit(maxId+data.count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e88218",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2611963741.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/fy/cjds5kvd3692g8__1nlvytj00000gn/T/ipykernel_50928/2611963741.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a12.withColumn(\"range\", array((1 to 100).map(lit(_)): _*)).show()\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a12.withColumn(\"range\", array((1 to 100).map(lit(_)): _*)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88985707",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.csv(\"/Users/arturo/repos/repos/test1/data-eng-scripts/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf36611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|833|\n",
      "|237|\n",
      "|856|\n",
      "|930|\n",
      "|871|\n",
      "|237|\n",
      "|849|\n",
      "|853|\n",
      "|237|\n",
      "|237|\n",
      "|237|\n",
      "|841|\n",
      "|847|\n",
      "|847|\n",
      "|886|\n",
      "|861|\n",
      "|833|\n",
      "|244|\n",
      "|859|\n",
      "|247|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.select(\"Id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369aeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301f40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c009d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad162de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606c3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9799a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cc7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34ca44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list  of employee data\n",
    "data = [[\"1\", \"sravan\", \"company 1\"],\n",
    "        [\"2\", \"ojaswi\", \"company 1\"],\n",
    "        [\"3\", \"rohith\", \"company 2\"],\n",
    "        [\"4\", \"sridevi\", \"company 1\"],\n",
    "        [\"5\", \"bobby\", \"company 1\"]]\n",
    " \n",
    "# specify column names\n",
    "columns = ['ID', 'NAME', 'Company']\n",
    " \n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "322713c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxid = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c47b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  Id|\n",
      "+----+\n",
      "|1001|\n",
      "|1002|\n",
      "|1003|\n",
      "|1004|\n",
      "|1005|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mylist = list(range(maxid+ 1,maxid + dataframe.count() + 1))\n",
    "\n",
    "# notice the parens after the type name\n",
    "index = spark.createDataFrame(mylist, IntegerType())\n",
    "index = index.withColumnRenamed(\"value\", \"Id\")\n",
    "\n",
    "index.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a44257ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn(\"Dummy\", monotonically_increasing_id())\n",
    "index = index.withColumn(\"Dummy\", monotonically_increasing_id())\n",
    "df3 = dataframe.join(index, \"Dummy\", \"outer\").drop(\"Dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e26be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+-----+\n",
      "| ID|   NAME|  Company|value|\n",
      "+---+-------+---------+-----+\n",
      "|  1| sravan|company 1| 1001|\n",
      "|  2| ojaswi|company 1| 1002|\n",
      "|  3| rohith|company 2| 1003|\n",
      "|  4|sridevi|company 1| 1004|\n",
      "|  5|  bobby|company 1| 1005|\n",
      "+---+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260e5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cb699a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fy/cjds5kvd3692g8__1nlvytj00000gn/T/ipykernel_50928/3618555263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1283\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1283\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\";\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mcommand_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \"\"\"\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "dataframe.withColumn(\"Index\", col(index.select(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecd96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05293d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   11|\n",
      "|   12|\n",
      "|   13|\n",
      "|   14|\n",
      "|   15|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b1166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ed822",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_1_new.union(df_2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96b20894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+--------------------+\n",
      "| ID|   NAME|  Company|               YEARS|\n",
      "+---+-------+---------+--------------------+\n",
      "|  1| sravan|company 1|[11, 12, 13, 14, 15]|\n",
      "|  2| ojaswi|company 1|[11, 12, 13, 14, 15]|\n",
      "|  3| rohith|company 2|[11, 12, 13, 14, 15]|\n",
      "|  4|sridevi|company 1|[11, 12, 13, 14, 15]|\n",
      "|  5|  bobby|company 1|[11, 12, 13, 14, 15]|\n",
      "+---+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76267ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fy/cjds5kvd3692g8__1nlvytj00000gn/T/ipykernel_50928/1353484913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Idl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3033\u001b[0m         \"\"\"\n\u001b[1;32m   3034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "dataframe.withColumn(\"Idl\", udf(lambda id: dates[id])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58199819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def06bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ef391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1).withColumn(\"YEARS\",  udf(lambda id: dates[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "601662b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'sequence(8199, 8203)'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence(lit(maxId+1), lit(maxId+dataframe.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28803a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8210"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78f8c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8198"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81f161d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "|2668|\n",
      "|4782|\n",
      "|4589|\n",
      "|3515|\n",
      "|2891|\n",
      "|7281|\n",
      "|2635|\n",
      "| 547|\n",
      "| 352|\n",
      "|8193|\n",
      "|5328|\n",
      "|6552|\n",
      "|1491|\n",
      "|1375|\n",
      "|6408|\n",
      "|2344|\n",
      "|2119|\n",
      "|8102|\n",
      "|1528|\n",
      "|6590|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fi4.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "173882cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(final.columns) - set(fi4.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfa2b1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8de29840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fi4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b7114f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C_ID_DEP_UD_PRINCIPAL              2\n",
       "CIF                                1\n",
       "codigoDir3                         1\n",
       "Provincia_Codigo                   1\n",
       "C_ID_UD_ORGANICA                   1\n",
       "C_DNM_UD_ORGANICA                  1\n",
       "C_ID_NIVEL_ADMON                   1\n",
       "C_ID_TIPO_ENT_PUBLICA              1\n",
       "N_NIVEL_JERARQUICO                 1\n",
       "C_ID_DEP_UD_SUPERIOR               1\n",
       "C_DNM_UD_ORGANICA_SUPERIOR         1\n",
       "C_DNM_UD_ORGANICA_PRINCIPAL        1\n",
       "B_SW_DEP_EDP_PRINCIPAL             1\n",
       "C_ID_DEP_EDP_PRINCIPAL             1\n",
       "C_DNM_UD_ORGANICA_EDP_PRINCIPAL    1\n",
       "C_ID_ESTADO                        1\n",
       "D_VIG_ALTA_OFICIAL                 1\n",
       "NIF_CIF                            1\n",
       "C_ID_AMB_PROVINCIA                 1\n",
       "C_DESC_PROV                        1\n",
       "CONTACTOS                          1\n",
       "List_Entidad_Norm                  1\n",
       "List_Provincia_Entidad             1\n",
       "codigoOrigen                       1\n",
       "NIF                                1\n",
       "Entidad_Norm                       1\n",
       "FormaJuridica_Descripcion          1\n",
       "Provincia_Entidad                  1\n",
       "ID_ENTIDAD                         1\n",
       "NIF_COD                            1\n",
       "ACRONIMO                           1\n",
       "NOMBRE_ENTIDAD                     1\n",
       "Nombre_Entidad_Mostrar             1\n",
       "TIPO_ENTIDAD_N1_1                  1\n",
       "TIPO_ENTIDAD_N2_1                  1\n",
       "DIRECCION_POSTAL                   1\n",
       "COD_POSTAL                         1\n",
       "COD_PROVINCIA                      1\n",
       "PROVINCIA                          1\n",
       "COD_CCAA                           1\n",
       "CCAA                               1\n",
       "ENLACE_WEB                         1\n",
       "SOMMA                              1\n",
       "TIPO_ENTIDAD_REGIONAL              1\n",
       "ESTADO_x                           1\n",
       "CodigoInvente                      1\n",
       "DenominacionSocial                 1\n",
       "FormaJuridica_Codigo               1\n",
       "List_CIF                           1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(final.columns)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f904c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba82d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "effddbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- CIF: string (nullable = true)\n",
      " |-- Entidad_Norm: string (nullable = true)\n",
      " |-- Provincia_Entidad: string (nullable = true)\n",
      " |-- ID_ENTIDAD: string (nullable = true)\n",
      " |-- NIF_COD: string (nullable = true)\n",
      " |-- ACRONIMO: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL: string (nullable = true)\n",
      " |-- COD_POSTAL: string (nullable = true)\n",
      " |-- COD_PROVINCIA: string (nullable = true)\n",
      " |-- PROVINCIA: string (nullable = true)\n",
      " |-- COD_CCAA: string (nullable = true)\n",
      " |-- CCAA: string (nullable = true)\n",
      " |-- ENLACE_WEB: string (nullable = true)\n",
      " |-- SOMMA: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL: string (nullable = true)\n",
      " |-- ESTADO_x: string (nullable = true)\n",
      " |-- CodigoInvente: string (nullable = true)\n",
      " |-- DenominacionSocial: string (nullable = true)\n",
      " |-- FormaJuridica_Codigo: string (nullable = true)\n",
      " |-- FormaJuridica_Descripcion: string (nullable = true)\n",
      " |-- NIF: string (nullable = true)\n",
      " |-- codigoDir3: string (nullable = true)\n",
      " |-- codigoOrigen: string (nullable = true)\n",
      " |-- Provincia_Codigo: string (nullable = true)\n",
      " |-- C_ID_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_ID_NIVEL_ADMON: string (nullable = true)\n",
      " |-- C_ID_TIPO_ENT_PUBLICA: string (nullable = true)\n",
      " |-- N_NIVEL_JERARQUICO: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_SUPERIOR: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_SUPERIOR: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_PRINCIPAL: string (nullable = true)\n",
      " |-- B_SW_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_ESTADO: string (nullable = true)\n",
      " |-- D_VIG_ALTA_OFICIAL: string (nullable = true)\n",
      " |-- NIF_CIF: string (nullable = true)\n",
      " |-- C_ID_AMB_PROVINCIA: string (nullable = true)\n",
      " |-- C_DESC_PROV: string (nullable = true)\n",
      " |-- CONTACTOS: string (nullable = true)\n",
      " |-- List_Entidad_Norm: string (nullable = true)\n",
      " |-- List_Provincia_Entidad: string (nullable = true)\n",
      " |-- List_CIF: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fi4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6021ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Entidad_Norm: string (nullable = true)\n",
      " |-- Provincia_Entidad: string (nullable = true)\n",
      " |-- ID_ENTIDAD: string (nullable = true)\n",
      " |-- NIF_COD: string (nullable = true)\n",
      " |-- ACRONIMO: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL: string (nullable = true)\n",
      " |-- COD_POSTAL: string (nullable = true)\n",
      " |-- COD_PROVINCIA: string (nullable = true)\n",
      " |-- PROVINCIA: string (nullable = true)\n",
      " |-- COD_CCAA: string (nullable = true)\n",
      " |-- CCAA: string (nullable = true)\n",
      " |-- ENLACE_WEB: string (nullable = true)\n",
      " |-- SOMMA: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL: string (nullable = true)\n",
      " |-- ESTADO_x: string (nullable = true)\n",
      " |-- CodigoInvente: string (nullable = true)\n",
      " |-- DenominacionSocial: string (nullable = true)\n",
      " |-- FormaJuridica_Codigo: string (nullable = true)\n",
      " |-- FormaJuridica_Descripcion: string (nullable = true)\n",
      " |-- NIF: string (nullable = true)\n",
      " |-- codigoDir3: string (nullable = true)\n",
      " |-- codigoOrigen: string (nullable = true)\n",
      " |-- Provincia_Codigo: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_ID_NIVEL_ADMON: string (nullable = true)\n",
      " |-- C_ID_TIPO_ENT_PUBLICA: string (nullable = true)\n",
      " |-- N_NIVEL_JERARQUICO: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_SUPERIOR: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_SUPERIOR: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_PRINCIPAL: string (nullable = true)\n",
      " |-- B_SW_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_ESTADO: string (nullable = true)\n",
      " |-- D_VIG_ALTA_OFICIAL: string (nullable = true)\n",
      " |-- NIF_CIF: string (nullable = true)\n",
      " |-- C_ID_AMB_PROVINCIA: string (nullable = true)\n",
      " |-- C_DESC_PROV: string (nullable = true)\n",
      " |-- CONTACTOS: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_PRINCIPAL: string (nullable = true)\n",
      " |-- List_Entidad_Norm: string (nullable = true)\n",
      " |-- List_Provincia_Entidad: string (nullable = true)\n",
      " |-- List_CIF: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280fe87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e22a42da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- CIF: string (nullable = true)\n",
      " |-- Entidad_Norm: string (nullable = true)\n",
      " |-- Provincia_Entidad: string (nullable = true)\n",
      " |-- ID_ENTIDAD: string (nullable = true)\n",
      " |-- NIF_COD: string (nullable = true)\n",
      " |-- ACRONIMO: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL: string (nullable = true)\n",
      " |-- COD_POSTAL: string (nullable = true)\n",
      " |-- COD_PROVINCIA: string (nullable = true)\n",
      " |-- PROVINCIA: string (nullable = true)\n",
      " |-- COD_CCAA: string (nullable = true)\n",
      " |-- CCAA: string (nullable = true)\n",
      " |-- ENLACE_WEB: string (nullable = true)\n",
      " |-- SOMMA: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL: string (nullable = true)\n",
      " |-- ESTADO_x: string (nullable = true)\n",
      " |-- CodigoInvente: string (nullable = true)\n",
      " |-- DenominacionSocial: string (nullable = true)\n",
      " |-- FormaJuridica_Codigo: string (nullable = true)\n",
      " |-- FormaJuridica_Descripcion: string (nullable = true)\n",
      " |-- NIF: string (nullable = true)\n",
      " |-- codigoDir3: string (nullable = true)\n",
      " |-- codigoOrigen: string (nullable = true)\n",
      " |-- Provincia_Codigo: string (nullable = true)\n",
      " |-- C_ID_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_ID_NIVEL_ADMON: string (nullable = true)\n",
      " |-- C_ID_TIPO_ENT_PUBLICA: string (nullable = true)\n",
      " |-- N_NIVEL_JERARQUICO: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_SUPERIOR: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_SUPERIOR: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_PRINCIPAL: string (nullable = true)\n",
      " |-- B_SW_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_ESTADO: string (nullable = true)\n",
      " |-- D_VIG_ALTA_OFICIAL: string (nullable = true)\n",
      " |-- NIF_CIF: string (nullable = true)\n",
      " |-- C_ID_AMB_PROVINCIA: string (nullable = true)\n",
      " |-- C_DESC_PROV: string (nullable = true)\n",
      " |-- CONTACTOS: string (nullable = true)\n",
      " |-- List_Entidad_Norm: string (nullable = true)\n",
      " |-- List_Provincia_Entidad: string (nullable = true)\n",
      " |-- List_CIF: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fi4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b45f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bb8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad37e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af7f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9898ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b915085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966de426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7024720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Entidad',\n",
       " 'Origen_Solicitud',\n",
       " 'Identificadores_Origen',\n",
       " 'Entidad_Norm',\n",
       " 'CIF',\n",
       " 'CIF_validacion',\n",
       " 'Centro',\n",
       " 'Centro_Norm',\n",
       " 'Tipo',\n",
       " 'Provincia_Entidad',\n",
       " 'Pais_Entidad',\n",
       " 'Provincia_Centro',\n",
       " 'PIC',\n",
       " 'Tipo_Persona',\n",
       " 'Entidad_Match',\n",
       " 'Provincia_Match',\n",
       " 'Match',\n",
       " 'Id_NIVEL_1',\n",
       " 'Id_NIVEL_0',\n",
       " 'ID_ENTIDAD_NIVEL_0',\n",
       " 'ID_ENTIDAD_NIVEL_1',\n",
       " 'Jerarquia',\n",
       " 'NIF_COD_NIVEL_0',\n",
       " 'ACRONIMO_NIVEL_0',\n",
       " 'NOMBRE_ENTIDAD_NIVEL_0',\n",
       " 'Nombre_Entidad_Mostrar_NIVEL_0',\n",
       " 'TIPO_ENTIDAD_N1_1_NIVEL_0',\n",
       " 'TIPO_ENTIDAD_N2_1_NIVEL_0',\n",
       " 'DIRECCION_POSTAL_NIVEL_0',\n",
       " 'COD_POSTAL_NIVEL_0',\n",
       " 'COD_PROVINCIA_NIVEL_0',\n",
       " 'PROVINCIA_NIVEL_0',\n",
       " 'COD_CCAA_NIVEL_0',\n",
       " 'CCAA_NIVEL_0',\n",
       " 'ENLACE_WEB_NIVEL_0',\n",
       " 'SOMMA_NIVEL_0',\n",
       " 'TIPO_ENTIDAD_REGIONAL_NIVEL_0',\n",
       " 'ESTADO_x_NIVEL_0',\n",
       " 'NIF_COD_NIVEL_1',\n",
       " 'ACRONIMO_NIVEL_1',\n",
       " 'NOMBRE_ENTIDAD_NIVEL_1',\n",
       " 'Nombre_Entidad_Mostrar_NIVEL_1',\n",
       " 'TIPO_ENTIDAD_N1_1_NIVEL_1',\n",
       " 'TIPO_ENTIDAD_N2_1_NIVEL_1',\n",
       " 'DIRECCION_POSTAL_NIVEL_1',\n",
       " 'COD_POSTAL_NIVEL_1',\n",
       " 'COD_PROVINCIA_NIVEL_1',\n",
       " 'PROVINCIA_NIVEL_1',\n",
       " 'COD_CCAA_NIVEL_1',\n",
       " 'CCAA_NIVEL_1',\n",
       " 'ENLACE_WEB_NIVEL_1',\n",
       " 'SOMMA_NIVEL_1',\n",
       " 'TIPO_ENTIDAD_REGIONAL_NIVEL_1',\n",
       " 'ESTADO_x_NIVEL_1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cruce1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe7963a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CIF', 'Entidad_Norm', 'Id', 'Provincia_Entidad'}\n"
     ]
    }
   ],
   "source": [
    "setA = set(cruce1.columns)\n",
    "setB = set(fi4.columns)\n",
    "\n",
    "print(setA & setB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b38f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'CIF',\n",
       " 'Entidad_Norm',\n",
       " 'Provincia_Entidad',\n",
       " 'ID_ENTIDAD',\n",
       " 'NIF_COD',\n",
       " 'ACRONIMO',\n",
       " 'NOMBRE_ENTIDAD',\n",
       " 'Nombre_Entidad_Mostrar',\n",
       " 'TIPO_ENTIDAD_N1_1',\n",
       " 'TIPO_ENTIDAD_N2_1',\n",
       " 'DIRECCION_POSTAL',\n",
       " 'COD_POSTAL',\n",
       " 'COD_PROVINCIA',\n",
       " 'PROVINCIA',\n",
       " 'COD_CCAA',\n",
       " 'CCAA',\n",
       " 'ENLACE_WEB',\n",
       " 'SOMMA',\n",
       " 'TIPO_ENTIDAD_REGIONAL',\n",
       " 'ESTADO_x',\n",
       " 'CodigoInvente',\n",
       " 'DenominacionSocial',\n",
       " 'FormaJuridica_Codigo',\n",
       " 'FormaJuridica_Descripcion',\n",
       " 'NIF',\n",
       " 'codigoDir3',\n",
       " 'codigoOrigen',\n",
       " 'Provincia_Codigo',\n",
       " 'C_ID_UD_ORGANICA',\n",
       " 'C_DNM_UD_ORGANICA',\n",
       " 'C_ID_NIVEL_ADMON',\n",
       " 'C_ID_TIPO_ENT_PUBLICA',\n",
       " 'N_NIVEL_JERARQUICO',\n",
       " 'C_ID_DEP_UD_SUPERIOR',\n",
       " 'C_DNM_UD_ORGANICA_SUPERIOR',\n",
       " 'C_ID_DEP_UD_PRINCIPAL',\n",
       " 'C_DNM_UD_ORGANICA_PRINCIPAL',\n",
       " 'B_SW_DEP_EDP_PRINCIPAL',\n",
       " 'C_ID_DEP_EDP_PRINCIPAL',\n",
       " 'C_DNM_UD_ORGANICA_EDP_PRINCIPAL',\n",
       " 'C_ID_ESTADO',\n",
       " 'D_VIG_ALTA_OFICIAL',\n",
       " 'NIF_CIF',\n",
       " 'C_ID_AMB_PROVINCIA',\n",
       " 'C_DESC_PROV',\n",
       " 'CONTACTOS',\n",
       " 'List_Entidad_Norm',\n",
       " 'List_Provincia_Entidad',\n",
       " 'List_CIF']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f184689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4e0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184d4742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Entidad: string (nullable = true)\n",
      " |-- Origen_Solicitud: string (nullable = true)\n",
      " |-- Identificadores_Origen: string (nullable = true)\n",
      " |-- Entidad_Norm: string (nullable = true)\n",
      " |-- CIF: string (nullable = true)\n",
      " |-- CIF_validacion: string (nullable = true)\n",
      " |-- Centro: string (nullable = true)\n",
      " |-- Centro_Norm: string (nullable = true)\n",
      " |-- Tipo: string (nullable = true)\n",
      " |-- Provincia_Entidad: string (nullable = true)\n",
      " |-- Pais_Entidad: string (nullable = true)\n",
      " |-- Provincia_Centro: string (nullable = true)\n",
      " |-- PIC: string (nullable = true)\n",
      " |-- Tipo_Persona: string (nullable = true)\n",
      " |-- Entidad_Match: string (nullable = true)\n",
      " |-- Provincia_Match: string (nullable = true)\n",
      " |-- Match: string (nullable = true)\n",
      " |-- Id_NIVEL_1: string (nullable = true)\n",
      " |-- Id_NIVEL_0: string (nullable = true)\n",
      " |-- ID_ENTIDAD_NIVEL_0: string (nullable = true)\n",
      " |-- ID_ENTIDAD_NIVEL_1: string (nullable = true)\n",
      " |-- Jerarquia: string (nullable = true)\n",
      " |-- NIF_COD_NIVEL_0: string (nullable = true)\n",
      " |-- ACRONIMO_NIVEL_0: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD_NIVEL_0: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar_NIVEL_0: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1_NIVEL_0: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1_NIVEL_0: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL_NIVEL_0: string (nullable = true)\n",
      " |-- COD_POSTAL_NIVEL_0: string (nullable = true)\n",
      " |-- COD_PROVINCIA_NIVEL_0: string (nullable = true)\n",
      " |-- PROVINCIA_NIVEL_0: string (nullable = true)\n",
      " |-- COD_CCAA_NIVEL_0: string (nullable = true)\n",
      " |-- CCAA_NIVEL_0: string (nullable = true)\n",
      " |-- ENLACE_WEB_NIVEL_0: string (nullable = true)\n",
      " |-- SOMMA_NIVEL_0: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL_NIVEL_0: string (nullable = true)\n",
      " |-- ESTADO_x_NIVEL_0: string (nullable = true)\n",
      " |-- NIF_COD_NIVEL_1: string (nullable = true)\n",
      " |-- ACRONIMO_NIVEL_1: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD_NIVEL_1: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar_NIVEL_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1_NIVEL_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1_NIVEL_1: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL_NIVEL_1: string (nullable = true)\n",
      " |-- COD_POSTAL_NIVEL_1: string (nullable = true)\n",
      " |-- COD_PROVINCIA_NIVEL_1: string (nullable = true)\n",
      " |-- PROVINCIA_NIVEL_1: string (nullable = true)\n",
      " |-- COD_CCAA_NIVEL_1: string (nullable = true)\n",
      " |-- CCAA_NIVEL_1: string (nullable = true)\n",
      " |-- ENLACE_WEB_NIVEL_1: string (nullable = true)\n",
      " |-- SOMMA_NIVEL_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL_NIVEL_1: string (nullable = true)\n",
      " |-- ESTADO_x_NIVEL_1: string (nullable = true)\n",
      " |-- Similitud: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruce1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bb041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58916892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- CIF: string (nullable = true)\n",
      " |-- Entidad_Norm: string (nullable = true)\n",
      " |-- Provincia_Entidad: string (nullable = true)\n",
      " |-- ID_ENTIDAD: string (nullable = true)\n",
      " |-- NIF_COD: string (nullable = true)\n",
      " |-- ACRONIMO: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL: string (nullable = true)\n",
      " |-- COD_POSTAL: string (nullable = true)\n",
      " |-- COD_PROVINCIA: string (nullable = true)\n",
      " |-- PROVINCIA: string (nullable = true)\n",
      " |-- COD_CCAA: string (nullable = true)\n",
      " |-- CCAA: string (nullable = true)\n",
      " |-- ENLACE_WEB: string (nullable = true)\n",
      " |-- SOMMA: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL: string (nullable = true)\n",
      " |-- ESTADO_x: string (nullable = true)\n",
      " |-- CodigoInvente: string (nullable = true)\n",
      " |-- DenominacionSocial: string (nullable = true)\n",
      " |-- FormaJuridica_Codigo: string (nullable = true)\n",
      " |-- FormaJuridica_Descripcion: string (nullable = true)\n",
      " |-- NIF: string (nullable = true)\n",
      " |-- codigoDir3: string (nullable = true)\n",
      " |-- codigoOrigen: string (nullable = true)\n",
      " |-- Provincia_Codigo: string (nullable = true)\n",
      " |-- C_ID_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA: string (nullable = true)\n",
      " |-- C_ID_NIVEL_ADMON: string (nullable = true)\n",
      " |-- C_ID_TIPO_ENT_PUBLICA: string (nullable = true)\n",
      " |-- N_NIVEL_JERARQUICO: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_SUPERIOR: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_SUPERIOR: string (nullable = true)\n",
      " |-- C_ID_DEP_UD_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_PRINCIPAL: string (nullable = true)\n",
      " |-- B_SW_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_DEP_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_DNM_UD_ORGANICA_EDP_PRINCIPAL: string (nullable = true)\n",
      " |-- C_ID_ESTADO: string (nullable = true)\n",
      " |-- D_VIG_ALTA_OFICIAL: string (nullable = true)\n",
      " |-- NIF_CIF: string (nullable = true)\n",
      " |-- C_ID_AMB_PROVINCIA: string (nullable = true)\n",
      " |-- C_DESC_PROV: string (nullable = true)\n",
      " |-- CONTACTOS: string (nullable = true)\n",
      " |-- List_Entidad_Norm: string (nullable = true)\n",
      " |-- List_Provincia_Entidad: string (nullable = true)\n",
      " |-- List_CIF: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fi4.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ecc6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id_NIVEL_1: string (nullable = true)\n",
      " |-- Id_NIVEL_0: string (nullable = true)\n",
      " |-- ID_ENTIDAD_NIVEL_0: string (nullable = true)\n",
      " |-- ID_ENTIDAD_NIVEL_1: string (nullable = true)\n",
      " |-- Jerarquia: string (nullable = true)\n",
      " |-- NIF_COD_NIVEL_0: string (nullable = true)\n",
      " |-- ACRONIMO_NIVEL_0: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD_NIVEL_0: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar_NIVEL_0: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1_NIVEL_0: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1_NIVEL_0: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL_NIVEL_0: string (nullable = true)\n",
      " |-- COD_POSTAL_NIVEL_0: string (nullable = true)\n",
      " |-- COD_PROVINCIA_NIVEL_0: string (nullable = true)\n",
      " |-- PROVINCIA_NIVEL_0: string (nullable = true)\n",
      " |-- COD_CCAA_NIVEL_0: string (nullable = true)\n",
      " |-- CCAA_NIVEL_0: string (nullable = true)\n",
      " |-- ENLACE_WEB_NIVEL_0: string (nullable = true)\n",
      " |-- SOMMA_NIVEL_0: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL_NIVEL_0: string (nullable = true)\n",
      " |-- ESTADO_x_NIVEL_0: string (nullable = true)\n",
      " |-- NIF_COD_NIVEL_1: string (nullable = true)\n",
      " |-- ACRONIMO_NIVEL_1: string (nullable = true)\n",
      " |-- NOMBRE_ENTIDAD_NIVEL_1: string (nullable = true)\n",
      " |-- Nombre_Entidad_Mostrar_NIVEL_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N1_1_NIVEL_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_N2_1_NIVEL_1: string (nullable = true)\n",
      " |-- DIRECCION_POSTAL_NIVEL_1: string (nullable = true)\n",
      " |-- COD_POSTAL_NIVEL_1: string (nullable = true)\n",
      " |-- COD_PROVINCIA_NIVEL_1: string (nullable = true)\n",
      " |-- PROVINCIA_NIVEL_1: string (nullable = true)\n",
      " |-- COD_CCAA_NIVEL_1: string (nullable = true)\n",
      " |-- CCAA_NIVEL_1: string (nullable = true)\n",
      " |-- ENLACE_WEB_NIVEL_1: string (nullable = true)\n",
      " |-- SOMMA_NIVEL_1: string (nullable = true)\n",
      " |-- TIPO_ENTIDAD_REGIONAL_NIVEL_1: string (nullable = true)\n",
      " |-- ESTADO_x_NIVEL_1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a12.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
